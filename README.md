 RAG Evaluation and Improvement System

 ğŸ“Œ Overview
This project implements a **Retrieval-Augmented Generation (RAG)** based
question answering system and evaluates its performance using quantitative
metrics. The system focuses on identifying weaknesses in RAG pipelines such as
poor retrieval, hallucinations, and inconsistent answers.

The project is designed as an **evaluation layer on top of RAG**, rather than
just another chatbot.


## ğŸ¯ Problem Statement
Most RAG systems are deployed without proper evaluation. This leads to:
- Irrelevant document retrieval
- Hallucinated answers
- Unstable responses for the same query

This project addresses the problem by **measuring and visualizing RAG quality**.

ğŸ§  System Architecture
1. Document ingestion  
2. Text chunking  
3. Embedding generation  
4. Vector search using FAISS  
5. Reranking retrieved documents  
6. Answer generation  
7. Evaluation using faithfulness and stability metrics  
8. Visualization using Streamlit dashboard  

 ğŸ“Š Evaluation Metrics

 Faithfulness
Measures how well the generated answer is supported by retrieved documents.
Low scores may indicate hallucinations.

Stability
Measures consistency of answers when the same query is asked multiple times.
Low stability indicates unreliable generation.

 ğŸ› ï¸ Tech Stack
- Python  
- Sentence Transformers  
- FAISS  
- HuggingFace Transformers  
- Streamlit  
- ngrok (for dashboard exposure)  

 ğŸ“ Project Structure

RAG-Evaluation-System/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs.txt
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ failures.jsonl
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

 â–¶ï¸ How to Run
 1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

 2ï¸âƒ£ Run Evaluation Pipeline
python main.py

 3ï¸âƒ£ Launch Dashboard
streamlit run app.py

(Optional) Use ngrok to expose the dashboard publicly.

 ğŸ“ˆ Output

* Evaluated answers with faithfulness and stability scores
* Logged failure cases
* Interactive dashboard showing evaluation results

 ğŸ“ Academic Relevance
This project demonstrates:
* Applied Natural Language Processing
* Vector similarity search
* Model evaluation techniques
* System-level design thinking

 ğŸ”® Future Work

* Add Recall@K metric for retrieval evaluation
* Compare multiple embedding models
* Automate chunk size optimization
* Deploy dashboard using Streamlit Cloud

