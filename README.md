# RAG Evaluation System

This project evaluates a Retrieval-Augmented Generation (RAG) question
answering system using faithfulness and stability metrics.

## Features
- Semantic document retrieval (FAISS)
- Answer reranking
- QA-based answer generation
- Evaluation metrics
- Streamlit dashboard
- ngrok deployment

## How to Run
1. Install dependencies: `pip install -r requirements.txt`
2. Run pipeline: `python main.py`
3. Launch dashboard: `streamlit run app.py`
